{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "DuplicateFlagError",
     "evalue": "The flag 'clf' is defined twice. First from E:\\Anaconda\\envs\\yolo\\lib\\site-packages\\ipykernel_launcher.py, Second from E:\\Anaconda\\envs\\yolo\\lib\\site-packages\\ipykernel_launcher.py.  Description from first occurrence: Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm, adclstm]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateFlagError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-64458ab7a4bb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Model choices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'adclstm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm, adclstm]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embd_file'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./data/word_embd.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"word embeding pickle file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFINE_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'char_embd_file'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./data/char_embd.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Char embeding pickle file\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\yolo\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     56\u001b[0m           \u001b[1;34m'Use of the keyword argument names (flag_name, default_value, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m           'docstring) is deprecated, please use (name, default, help) instead.')\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0moriginal_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\yolo\\lib\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE_string\u001b[1;34m(name, default, help, flag_values, **args)\u001b[0m\n\u001b[0;32m    239\u001b[0m   \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentParser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m   \u001b[0mserializer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_argument_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArgumentSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m   \u001b[0mDEFINE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\yolo\\lib\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE\u001b[1;34m(parser, name, default, help, flag_values, serializer, module_name, **args)\u001b[0m\n\u001b[0;32m     80\u001b[0m   \"\"\"\n\u001b[0;32m     81\u001b[0m   DEFINE_flag(_flag.Flag(parser, serializer, name, default, help, **args),\n\u001b[1;32m---> 82\u001b[1;33m               flag_values, module_name)\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\yolo\\lib\\site-packages\\absl\\flags\\_defines.py\u001b[0m in \u001b[0;36mDEFINE_flag\u001b[1;34m(flag, flag_values, module_name)\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[1;31m# Copying the reference to flag_values prevents pychecker warnings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m   \u001b[0mfv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m   \u001b[0mfv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m   \u001b[1;31m# Tell flag_values who's defining the flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda\\envs\\yolo\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, name, flag)\u001b[0m\n\u001b[0;32m    428\u001b[0m         \u001b[1;31m# module is simply being imported a subsequent time.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0m_exceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDuplicateFlagError\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_flag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[0mshort_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshort_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;31m# If a new flag overrides an old one, we need to cleanup the old flag's\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDuplicateFlagError\u001b[0m: The flag 'clf' is defined twice. First from E:\\Anaconda\\envs\\yolo\\lib\\site-packages\\ipykernel_launcher.py, Second from E:\\Anaconda\\envs\\yolo\\lib\\site-packages\\ipykernel_launcher.py.  Description from first occurrence: Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm, adclstm]"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import pickle as pkl\n",
    "\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "# from tensorflow.python import debug as tf_debug\n",
    "\n",
    "import data_helper\n",
    "# from rnn_classifier import rnn_clf\n",
    "# from cnn_classifier import cnn_clf\n",
    "# from attn_rnn_classifier import attn_rnn_clf\n",
    "# from clstm_classifier import clstm_clf\n",
    "from double_aclstm_classifier import double_aclstm_clf as clstm_clf\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "except ImportError as e:\n",
    "    error = \"Please install scikit-learn.\"\n",
    "    print(str(e) + ': ' + error)\n",
    "    sys.exit()\n",
    "\n",
    "# Show warnings and errors only\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Parameters\n",
    "# =============================================================================\n",
    "\n",
    "# Model choices\n",
    "tf.flags.DEFINE_string('clf', 'adclstm', \"Type of classifiers. Default: cnn. You have four choices: [cnn, lstm, blstm, clstm, adclstm]\")\n",
    "tf.flags.DEFINE_string('embd_file', './data/word_embd.pickle', \"word embeding pickle file\")\n",
    "tf.flags.DEFINE_string('char_embd_file', './data/char_embd.pickle', \"Char embeding pickle file\")\n",
    "\n",
    "# Data parameters\n",
    "tf.flags.DEFINE_string('data_file', \"./data/model_input_word_data.txt\", 'Data file path')\n",
    "tf.flags.DEFINE_string('char_data_file', \"./data/model_input_char_data.txt\", 'Char Data file path')\n",
    "tf.flags.DEFINE_string('stop_word_file', None, 'Stop word file path')\n",
    "tf.flags.DEFINE_string('language', 'en', \"Language of the data file. You have two choices: [ch, en]\")\n",
    "tf.flags.DEFINE_integer('min_frequency', 0, 'Minimal word frequency')\n",
    "tf.flags.DEFINE_integer('num_classes', 12, 'Number of classes')\n",
    "tf.flags.DEFINE_integer('max_length', 28, 'Max document length')\n",
    "tf.flags.DEFINE_integer('char_max_length', 51, 'Char Max document length')\n",
    "tf.flags.DEFINE_integer('vocab_size', 0, 'Vocabulary size')\n",
    "tf.flags.DEFINE_float('test_size', 0.2, 'Cross validation test size')\n",
    "\n",
    "# Model hyperparameters\n",
    "tf.flags.DEFINE_integer('embedding_size', 200, 'Word embedding size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('char_embedding_size', 200, 'Word embedding size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_string('filter_sizes', '3, 4, 5', 'CNN filter sizes. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('num_filters', 128, 'Number of filters per filter size. For CNN, C-LSTM.')\n",
    "tf.flags.DEFINE_integer('hidden_size', 100, 'Number of hidden units in the LSTM cell. For LSTM, Bi-LSTM')\n",
    "tf.flags.DEFINE_integer('num_layers', 2, 'Number of the LSTM cells. For LSTM, Bi-LSTM, C-LSTM')\n",
    "tf.flags.DEFINE_float('keep_prob', 0.5, 'Dropout keep probability')  # All\n",
    "tf.flags.DEFINE_float('learning_rate', 1e-4, 'Learning rate')  # All\n",
    "tf.flags.DEFINE_float('l2_reg_lambda', 0.001, 'L2 regularization lambda')  # All\n",
    "\n",
    "tf.flags.DEFINE_integer('attn_size', 200, 'attention layer size')\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer('batch_size', 100, 'Batch size')\n",
    "tf.flags.DEFINE_integer('num_epochs', 50, 'Number of epochs')\n",
    "tf.flags.DEFINE_integer('evaluate_every_steps', 100, 'Evaluate the model on validation set after this many steps')\n",
    "tf.flags.DEFINE_integer('save_every_steps', 4000, 'Save the model after this many steps')\n",
    "tf.flags.DEFINE_integer('num_checkpoint', 10, 'Number of models to store')\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "\n",
    "if FLAGS.clf == 'lstm':\n",
    "    FLAGS.embedding_size = FLAGS.hidden_size\n",
    "elif FLAGS.clf == 'clstm' or FLAGS.clf == 'adclstm':\n",
    "    FLAGS.hidden_size = len(FLAGS.filter_sizes.split(\",\")) * FLAGS.num_filters\n",
    "\n",
    "# Output files directory\n",
    "timestamp = str(int(time.time()))\n",
    "outdir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "\n",
    "# Load and save data\n",
    "# =============================================================================\n",
    "\n",
    "data, labels = data_helper.load_data(file_path=FLAGS.data_file,\n",
    "                                                               sw_path=FLAGS.stop_word_file,\n",
    "                                                               min_frequency=FLAGS.min_frequency,\n",
    "                                                               max_length=FLAGS.max_length,\n",
    "                                                               language=FLAGS.language,\n",
    "                                                               shuffle=True)\n",
    "\n",
    "char_data, char_labels = data_helper.load_data(file_path=FLAGS.char_data_file,\n",
    "                                                               sw_path=FLAGS.stop_word_file,\n",
    "                                                               min_frequency=FLAGS.min_frequency,\n",
    "                                     max_length=FLAGS.char_max_length,\n",
    "                                                               language=FLAGS.language,\n",
    "                                                           shuffle=True)\n",
    "\n",
    "union_data = list()\n",
    "for i in range(0, len(data)):\n",
    "    tmp = [data[i], char_data[i]]\n",
    "    union_data.append(tmp)\n",
    "\n",
    "params = FLAGS.__flags\n",
    "# Print parameters\n",
    "model = params['clf']\n",
    "if model == 'cnn':\n",
    "    del params['hidden_size']\n",
    "    del params['num_layers']\n",
    "elif model == 'lstm' or model == 'blstm':\n",
    "    del params['num_filters']\n",
    "    del params['filter_sizes']\n",
    "    params['embedding_size'] = params['hidden_size']\n",
    "elif model == 'clstm' or model == 'adclstm':\n",
    "    params['hidden_size'] = len(list(map(int, params['filter_sizes'].split(\",\")))) * params['num_filters']\n",
    "\n",
    "params_dict = sorted(params.items(), key=lambda x: x[0])\n",
    "print('Parameters:')\n",
    "for item in params_dict:\n",
    "    print('{}: {}'.format(item[0], item[1]))\n",
    "print('')\n",
    "\n",
    "# Save parameters to file\n",
    "params_file = open(os.path.join(outdir, 'params.pkl'), 'wb')\n",
    "pkl.dump(params, params_file, True)\n",
    "params_file.close()\n",
    "\n",
    "\n",
    "# Simple Cross validation\n",
    "# TODO use k-fold cross validation\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(union_data,\n",
    "                                                    labels,\n",
    "                                                    test_size=FLAGS.test_size,\n",
    "                                                    random_state=0)\n",
    "# Batch iterator\n",
    "train_data = data_helper.batch_iter(x_train, y_train, FLAGS.batch_size, FLAGS.num_epochs, FLAGS.max_length,\n",
    "                                    FLAGS.char_max_length)\n",
    "\n",
    "# Train\n",
    "# =============================================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    # config = tf.ConfigProto()\n",
    "    # config.gpu_options.allocator_type = 'BFC'\n",
    "    with tf.Session() as sess:\n",
    "        # sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "\n",
    "        # sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "        if FLAGS.clf == 'cnn':\n",
    "            classifier = cnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'lstm' or FLAGS.clf == 'blstm':\n",
    "            classifier = rnn_clf(FLAGS)\n",
    "        elif FLAGS.clf == 'clstm' or FLAGS.clf == 'adclstm':\n",
    "            classifier = clstm_clf(FLAGS)\n",
    "        elif FLAGS.clf == \"attn_lstm\":\n",
    "            classifier = attn_rnn_clf(FLAGS)\n",
    "        else:\n",
    "            raise ValueError('clf should be one of [cnn, lstm, blstm, clstm, adclstm]')\n",
    "\n",
    "        # Train procedure\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(FLAGS.learning_rate)\n",
    "        grads_and_vars = optimizer.compute_gradients(classifier.cost)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Summaries\n",
    "        loss_summary = tf.summary.scalar('Loss', classifier.cost)\n",
    "        accuracy_summary = tf.summary.scalar('Accuracy', classifier.accuracy)\n",
    "\n",
    "        # Train summary\n",
    "        train_summary_op = tf.summary.merge_all()\n",
    "        train_summary_dir = os.path.join(outdir, 'summaries', 'train')\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Validation summary\n",
    "        valid_summary_op = tf.summary.merge_all()\n",
    "        valid_summary_dir = os.path.join(outdir, 'summaries', 'valid')\n",
    "        valid_summary_writer = tf.summary.FileWriter(valid_summary_dir, sess.graph)\n",
    "\n",
    "        saver = tf.train.Saver(max_to_keep=FLAGS.num_checkpoint)\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "        def run_step(input_data, is_training=True):\n",
    "            \"\"\"Run one step of the training process.\"\"\"\n",
    "            input_x, input_char_x, input_y, sequence_length, char_sequence_length = input_data\n",
    "\n",
    "            fetches = {'step': global_step,\n",
    "                       'cost': classifier.cost,\n",
    "                       'accuracy': classifier.accuracy}\n",
    "            feed_dict = {classifier.input_x: input_x,\n",
    "                         classifier.input_char_x: input_char_x,\n",
    "                         classifier.input_y: input_y}\n",
    "\n",
    "            fetches['correct_num'] = classifier.correct_num\n",
    "            fetches['predictions'] = classifier.predictions\n",
    "            # fetches['input_y'] = classifier.input_y\n",
    "            if FLAGS.clf != 'cnn':\n",
    "                fetches['final_state'] = classifier.final_state\n",
    "                feed_dict[classifier.batch_size] = len(input_x)\n",
    "                feed_dict[classifier.sequence_length] = sequence_length\n",
    "                feed_dict[classifier.char_sequence_length] = char_sequence_length\n",
    "\n",
    "            if is_training:\n",
    "                fetches['train_op'] = train_op\n",
    "                fetches['summaries'] = train_summary_op\n",
    "                feed_dict[classifier.keep_prob] = FLAGS.keep_prob\n",
    "            else:\n",
    "                fetches['summaries'] = valid_summary_op\n",
    "\n",
    "                feed_dict[classifier.keep_prob] = 1.0\n",
    "\n",
    "            vars = sess.run(fetches, feed_dict)\n",
    "            step = vars['step']\n",
    "            cost = vars['cost']\n",
    "            accuracy = vars['accuracy']\n",
    "            summaries = vars['summaries']\n",
    "            correct_num = vars['correct_num']\n",
    "            predictions = vars['predictions']\n",
    "            # correct_num = vars['correct_num']\n",
    "\n",
    "            # Write summaries to file\n",
    "            if is_training:\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "            else:\n",
    "                valid_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            if is_training:\n",
    "                print(\"{}: step: {}, loss: {:g}, accuracy: {:g}, num: {}\".format(time_str, step, cost, accuracy, correct_num))\n",
    "\n",
    "            return correct_num, predictions, input_y\n",
    "\n",
    "\n",
    "        print('Start training ...')\n",
    "\n",
    "        for train_input in train_data:\n",
    "            run_step(train_input, is_training=True)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "\n",
    "            #if current_step % FLAGS.evaluate_every_steps == 0:\n",
    "\n",
    "\n",
    "            # if current_step % FLAGS.save_every_steps == 0:\n",
    "            #    save_path = saver.save(sess, os.path.join(outdir, 'model/clf'), current_step)\n",
    "        print('\\nValidation')\n",
    "        valid_data = data_helper.batch_iter(x_valid, y_valid, FLAGS.batch_size, 1, FLAGS.max_length, FLAGS.char_max_length)\n",
    "        correct_num = 0\n",
    "        total_predictions = list()\n",
    "        total_input_y = list()\n",
    "        for valid_input in valid_data:\n",
    "            cur_correct_num, predictions, input_y = run_step(valid_input, is_training=False)\n",
    "            correct_num += cur_correct_num\n",
    "            total_predictions.extend(predictions)\n",
    "            total_input_y.extend(input_y)\n",
    "        correct_num /= len(x_valid)\n",
    "        recall = sklearn.metrics.recall_score(total_input_y, total_predictions, average=\"macro\")\n",
    "        print('END:%g' % correct_num)\n",
    "        print(\"recall:%g\" % recall)\n",
    "\n",
    "        saver.save(sess, os.path.join(outdir, 'model/clf'), 1)\n",
    "        print('\\nAll the files have been saved to {}\\n'.format(outdir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
